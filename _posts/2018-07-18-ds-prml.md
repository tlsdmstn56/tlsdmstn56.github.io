---
layout: single
title: "[정리] PRML 새로 알게 된 것"
tags: ['machine learning', 'stat']
date: 2018-07-18 01:00:00
---

Pattern Recognition and Machine Learning이 정리 된 [사이트](http://norman3.github.io/prml/)을 보면서 새로 알게 된 것들을 정리해 보았습니다. 지극히 개인적인 기준으로 적은 것이니 아마 저만 보게 될 것 같습니다.

## 클래스 판별 문제 종류

* Generative Models
  * 주어진 데이터를 통해 모델링 된 분포로부터 완전히 새로운 샘플들을 다시 생성 가능
* Discriminative Models
  * 사후 확률을 직접 모델링
* Discriminant Function
  * 사후 확률을 따지지 않음



## 정보 이론

### 엔트로피

놀람의 정도(degree of surprise)
$$
H[x] = -\sum_x \ p(x) \log_2p(x)
$$
엔트로피: 평균 정보량, $p(x)$인 분포에서 $p(x)$ 함수의 기대값을 의미미

### 최대 엔트로피

$p(x)$가 특정 지점에서 높게 피크를 치면 낮은 엔트로피를 얻게 된다.

Non-uniform 분포가 Uniform 분포보다 엔트로피가 낮다

엔트로피를 최대로 하는 확률은 정규분포를 따른다.

### 조건부 엔트로피

$$
H[\bold{x},\bold{y}] = H[\bold{y}|\bold{x}]+H[\bold{x}]
$$

- $(\bold{x},\bold{y})$ 를 기술하기 위해 필요한 정보는 $\bold{x}$ 만 기술하기 위한 정보와
- $\bold{x}$ 가 주어졌을 때 $\bold{y}$ 를 기술하기 위한 정보의 합으로 이루어진다.
- 곱이 아니라 합이라는 사실이 중요한데, 사실 내부적으로 로그가 포함되어 있기 때문.

## KL Divergence(relative entropy)

$$
\begin{aligned}
KL(p||q) &= -\int p(\bold{x}) \ ln \ q(\bold(x))  \ d\bold{x} - \bigg(-\int p(\bold{x}) \ ln \ p(\bold(x))  \ d\bold{x} \bigg) \\
&=-\int p(\bold{x}) \ ln \ \frac{q(\bold(x))}{p(\bold(x))}  \ d\bold{x}
\end{aligned}
$$

성질

* $KL(p||q)  \neq KL(q||p) $
* $$p(\bold{x})=q(\bold{x}) \ \ \Leftrightarrow  \ \ KL(p||q)=0$$



## 상호정보(Mutual Information)

$$
I[{\bf x}, {\bf y}] \equiv KL(p({\bf x}, {\bf y})||p({\bf x})p({\bf y})) = - \iint p({\bf x}, {\bf y})\ln\left(\dfrac{p({\bf x})p({\bf y})}{p({\bf x}, {\bf y})}\right)d{\bf x}d{\bf y}
$$

- 항상 \\( I[{\bf x}, {\bf y}] \ge 0 \\)을 만족한다.
- 만약 \\( {\bf x} \\) 와 \\( {\bf y} \\)가 서로 독립적이라면 \\( I[{\bf x}, {\bf y}] = 0 \\) 이 된다.
- 합(sum) 과 곱(product)의 법칙도 사용할 수 있다.

$$I[{\bf x}, {\bf y}] = H[{\bf x}] - H[{\bf x}|{\bf y}] = H[{\bf y}] - H[{\bf y}|{\bf x}]$$

- *mutual information* 은 조건부 엔트로피(conditional entropy)와 연관이 있다.
- $\bold{y}$ 를 알고 난 후에 $\bold{x}$의 불확실성을 줄이는 과정으로 볼 수 있다.
- 베이지안 관점으로 접근하자면 $ p({\bf x}) $ 는 사전(prior) 분포, $ p({\bf x}|{\bf y}) $ 는 관찰 값 $ {\bf y} $ 를 측정한 이후의 사후(posterior) 분포로 볼 수 있다.

